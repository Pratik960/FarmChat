{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76f95088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "# from tensorflow.keras.optimizers import SGD\n",
    "# import tensorflow.keras.legacy.optimizers.SGD\n",
    "import tensorflow as tf\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd284580",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json',encoding=\"utf-8\").read()\n",
    "intents = json.loads(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56270b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "\n",
    "        #tokenize each word\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        words.extend(w)\n",
    "        #add documents in the corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc0e6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9598e478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 documents\n",
      "52 classes ['\\tPlant Protection\\t', 'Fertilizer Use and Availability', 'General_About_You', 'General_Agent_Capabilities', 'General_Connect_to_Agent', 'General_Ending', 'General_Greetings', 'General_Human_or_Bot', 'General_Negative_Feedback', 'General_Positive_Feedback', 'Income-based-policies', 'Kisan-Credit-Card', 'Loan-policies', 'Medicinal and Aromatic Plants', 'Nutrient Management', 'PM-Kisan-Maandhan-Scheme', 'Pesticides-disease-downy-mildew', 'Rainfed-Area-Development-Programme', 'Seeds and Planting Material', 'Symptoms-disease-downy-mildew', 'Varieties', 'activities-in-pradhan-mantri-krishi-sinchayee-yojana', 'brands-for-bayleton', 'brands-for-bordeaux-mixture', 'brands-for-carbendazim', 'brands-for-copper-oxychloride', 'brands-for-fosetyl', 'brands-for-mancozeb', 'brands-for-metalaxyl', 'brands-for-streptocycline', 'brands-for-wettable sulphur', 'choice-brands', 'choice-diseases', 'choice-gov-policies', 'choice-pest', 'disease-anthracnose', 'disease-bacterial-leaf-spot', 'disease-downy-mildew', 'disease-powder-mildew', 'financial-assistance-in-paramparagat-krishi-vikas-yojana', 'insurance-policies', 'market-policies', 'more', 'national-agriculture-market', 'national-mission-for-sustainable-agriculture', 'paramparagat-krishi-vikas-yojana', 'pesticides-disease-anthracnose', 'pradhan-mantri-krishi-sinchayee-yojana', 'soil-health-cards', 'soil-health-policies', 'sustainable-and-organic-farming-policies', 'terminal-markets-complex']\n",
      "838 unique lemmatized words [\"'d\", \"'m\", \"'re\", \"'s\", \"'ve\", '(', ')', ',', '-onion', '.', '4', '5', ':', 'a', 'able', 'about', 'activity', 'actually', 'advisor', 'affected', 'again', 'agent', 'agriculture', 'ai', 'all', 'am', 'amount', 'an', 'and', 'annoying', 'answer', 'anthracnose', 'any', 'anyone', 'anything', 'are', 'area', 'ask', 'asked', 'asking', 'assist', 'assistance', 'at', 'attack', 'away', 'awesome', 'back', 'bacterial', 'based', 'bayleton', 'been', 'being', 'believe', 'bengal', 'best', 'bima', 'blight', 'bordeaux-mixture', 'boring', 'born', 'bot', 'brand', 'brilliant', 'by', 'bye', 'ca', 'call', 'campestris', 'can', 'capable', 'carbendazim', 'card', 'catch', 'chatting', 'closest', 'company', 'complex', 'computer', 'configure', 'connect', 'contact', 'control', 'cool', 'copper', 'could', 'credit', 'crop', 'cultivation', 'customer', 'cya', 'd', 'day', 'deal', 'delusion', 'describe', 'development', 'did', 'disease', 'diseasees', 'distribution', 'do', 'doing', 'done', 'dose', 'downy', 'else', 'elsinoeampelina', 'enam', 'end', 'ending', 'eve', 'even', 'evening', 'everyone', 'everything', 'exactly', 'farm', 'farmer', 'farming', 'feature', 'fertile', 'fertilizer', 'finance', 'financial', 'find', 'finished', 'for', 'forward', 'fosetyl', 'frequently', 'from', 'frustrating', 'full', 'function', 'gave', 'genius', 'get', 'give', 'given', 'go', 'going', 'good', 'goodbye', 'got', 'gov', 'government', 'gram', 'gram/chick', 'great', 'greeting', 'groundnut', 'growth', 'handle', 'hat', 'hate', 'have', 'having', 'health', 'hello', 'help', 'helpful', 'here', 'hey', 'hi', 'how', 'human', 'i', 'im', 'in', 'income', 'info', 'insurance', 'insurance-based', 'introduce', 'irrigation', 'irritating', 'is', 'it', 'jal', 'jassid', 'kcc', 'keep', 'khet', 'kind', 'kisan', 'know', 'ko', 'krishi', 'language', 'later', 'leaf', 'leaving', 'let', 'life', 'like', 'list', 'live', 'loan', 'looking', 'lost', 'love', 'maandhan', 'madat', 'made', 'maize', 'make', 'makka', 'man', 'management', 'manager', 'mancozeb', 'mantri', 'many', 'market', 'market-based', 'master', 'me', 'menu', 'metalaxyl', 'mildew', 'minister', 'mission', 'money', 'more', 'morning', 'much', 'my', \"n't\", 'name', 'national', 'necator', 'need', 'nerve', 'new', 'nice', 'nmsa', 'no', 'not', 'now', 'nut/mung', 'nutrient', 'of', 'off', 'offer', 'ok', 'old', 'on', 'onion', 'operator', 'option', 'or', 'organic', 'other', 'out', 'oxychloride', 'pani', 'paramparagat', 'pea', 'pea/kabuli/chana', 'person', 'pesticide', 'phallus', 'pkvy', 'plant', 'plasmoparaviticola', 'please', 'pls', 'pm', 'pm-kisan', 'pmksy', 'policy', 'powder', 'pradhan', 'prevent', 'prime', 'problem', 'producing', 'programme', 'prove', 'provide', 'provided', 'put', 'question', 'quit', 'r', 'radp', 'rainfed', 'real', 'really', 'recorded', 'representative', 'robot', 'saffron', 'sanchay', 'scheme', 'see', 'seem', 'self', 'send', 'service', 'session', 'should', 'shut', 'sinchayee', 'sinchayi', 'skill', 'skip', 'smart', 'so', 'soil', 'solution', 'solve', 'some', 'somebody', 'someone', 'something', 'speak', 'spot', 'spray', 'stop', 'straight', 'streptocycline', 'stupid', 'sudhar', 'sulphur', 'sustainable', 'sustainable/organic', 'suvidha', 'symptom', 'take', 'talk', 'talking', 'tell', 'terminal', 'thank', 'thanks', 'that', 'the', 'there', 'thing', 'this', 'thrip', 'through', 'time', 'tmc', 'to', 'today', 'too', 'transfer', 'trial', 'twin', 'type', 'u', 'uncinula', 'understand', 'up', 'use', 'used', 'variety', 'very', 'vikas', 'wa', 'want', 'water', 'waterlogged', 'watermelon', 'watson', 'weed', 'well', 'were', 'wet', 'wettable', 'what', 'wheat', 'where', 'which', 'who', 'why', 'with', 'wonderful', 'work', 'working', 'would', 'xanthomonas', 'yes', 'yojana', 'you', 'your', 'yourself', 'અજમાયશ', 'અટકાવવું', 'અત્યારે', 'અદ્ભુત', 'અનસિનુલા', 'અને', 'અન્ય', 'અમુક', 'અરે', 'અસર', 'અહી', 'અહીં', 'અહીંથી', 'આ', 'આજે', 'આટલા', 'આટલું', 'આતુર', 'આધારિત', 'આનંદ', 'આપવામાં', 'આપી', 'આપે', 'આપો', 'આપ્યું', 'આભાર', 'આરએડીપી', 'આરોગ્ય', 'આવક', 'આવજો', 'આવું', 'આવે', 'આવેલ', 'ઇવન', 'ઉંમર', 'ઉત્પાદન', 'ઉપયોગ', 'એક', 'એજન્ટ', 'એટલા', 'એન્થ્રેકનોઝ', 'એન્થ્રેકનોઝનું', 'એન્થ્રેકનોઝને', 'એવી', 'એવું', 'ઓ', 'ઓક્સીક્લોરાઇડ', 'ઓક્સીક્લોરાઇડનું', 'ઓપરેટર', 'ઓફર', 'ઓર્ગેનિક', 'કંઈ', 'કંઈક', 'કંઈપણ', 'કંટાળાજનક', 'કંપનીઓ', 'કઈ', 'કનેક્ટ', 'કયા', 'કરતી', 'કરતો', 'કરવા', 'કરવાની', 'કરવાનું', 'કરવામાં', 'કરવી', 'કરી', 'કરીએ', 'કરીને', 'કરીશ', 'કરુ', 'કરું', 'કરે', 'કરેલ', 'કરો', 'કર્યું', 'કહી', 'કહો', 'કામ', 'કામને', 'કાર્ડ', 'કાર્ડ્સ', 'કાર્બેન્ડાઝીમ', 'કાર્બેન્ડાઝીમનું', 'કાર્ય', 'કાર્યક્રમ', 'કિસાન', 'કુશળતા', 'કૃપા', 'કૃષિ', 'કે', 'કેટલા', 'કેટલાક', 'કેટલી', 'કેટલીક', 'કેટલું', 'કેમ', 'કેમ્પેસ્ટ્રીસ', 'કેવા', 'કેવી', 'કેવુ', 'કેસીસી', 'કો', 'કોઈ', 'કોઈની', 'કોઈપણ', 'કોણ', 'કોપર', 'કોમ્પ્યુટર', 'કોમ્પ્લેક્સ', 'કોલ', 'ક્યા', 'ક્યાં', 'ક્યાંથી', 'ક્રેડિટ', 'ખબર', 'ખરેખર', 'ખૂબ', 'ખેડૂતો', 'ખેડૂતોને', 'ખેત', 'ખેતી', 'ખેતીની', 'ગમે', 'ગુડ', 'ગોઠવી', 'ગ્રાહક', 'ચાલે', 'ચાલો', 'ચાલ્યો', 'ચિડાઈ', 'ચુપ', 'ચેટ', 'ચેતા', 'છીએ', 'છુ', 'છું', 'છે', 'છો', 'છોડને', 'છોડવા', 'છોડી', 'જ', 'જંતુનાશક', 'જંતુનાશકો', 'જઈ', 'જઈએ', 'જઉં', 'જણાવો', 'જતા', 'જન્મ', 'જમીનની', 'જમીનને', 'જરૂર', 'જલ', 'જવા', 'જવાનો', 'જવાબ', 'જવું', 'જા', 'જાઉં', 'જાઓ', 'જાણવા', 'જાણવાની', 'જાણવું', 'જાણી', 'જાણો', 'જાતને', 'જાળવવા', 'જીવંત', 'જીવન', 'જુઓ', 'જે', 'જેના', 'જેની', 'જેમાં', 'જૈવિક', 'જોઈએ', 'જોઈતો', 'જોઈને', 'જોડાઈ', 'જોડાવા', 'જોડિયા', 'જોડી', 'જોડો', 'ઝેન્થોમોનાસ', 'ટકાઉ', 'ટકાઉ/ઓર્ગેનિક', 'ટર્મિનલ', 'ટીએમસી', 'ટ્રાન્સફર', 'ઠીક', 'ડાઉની', 'તંદુરસ્તી', 'તત્વોથી', 'તને', 'તમને', 'તમામ', 'તમારા', 'તમારી', 'તમારું', 'તમારો', 'તમે', 'તારી', 'તે', 'તેજસ્વી', 'ત્યાં', 'થયા', 'થયો', 'થાઓ', 'થાય', 'થોડી', 'થોડું', 'દરેક', 'દિવસ', 'દૂર', 'દો', 'દ્વારા', 'ધિક્કાર', 'ધિક્કારે', 'નજીકનો', 'નથી', 'નફરત', 'નમસ્તે', 'નવું', 'ના', 'નાણાં', 'નાણાંની', 'નાણાકીય', 'નાણાની', 'નામ', 'નિરાશાજનક', 'નીતિ', 'નીતિઓ', 'નેકેટર', 'પછી', 'પણ', 'પર', 'પરમપરાગત', 'પસંદ', 'પહોંચાડો', 'પાછા', 'પાડવામાં', 'પાણી', 'પાની', 'પાવડર', 'પાવડરી', 'પાસે', 'પીએમ', 'પીએમ-કિસાન', 'પીએમકેએસવાય', 'પીકેવીવાય', 'પૂછવું', 'પૂછાતા', 'પૂરા', 'પૈસા', 'પૉલિસી', 'પોલિસી', 'પોષક', 'પ્રકારના', 'પ્રકારની', 'પ્રકારો', 'પ્રતિનિધિ', 'પ્રતિભાશાળી', 'પ્રધાનમંત્રી', 'પ્રવૃત્તિઓ', 'પ્રશ્નો', 'પ્રશ્નોના', 'પ્રેમ', 'પ્લાઝમોપેરાવિટીકોલા', 'પ્લીઝ', 'ફરી', 'ફળદ્રુપ', 'ફાઇનાન્સ', 'ફોસેટાઇલ', 'ફોસેટાઇલનું', 'બંધ', 'બજાર', 'બધા', 'બધું', 'બનાવવા', 'બનાવવામાં', 'બરાબર', 'બહાર', 'બહુ', 'બાબતો', 'બાય', 'બીજું', 'બેક્ટેરિયલ', 'બેલેટન', 'બેલેટનનું', 'બોટ', 'બોર્ડેક્સ-મિશ્રણ', 'બોર્ડેક્સ-મિશ્રણનું', 'બોલો', 'બ્રાન્ડ્સ', 'ભરપૂર', 'ભરાયેલી', 'ભાષાઓ', 'ભીની', 'ભ્રમણા', 'મદદ', 'મદદની', 'મદદરૂપ', 'મને', 'મળીશું', 'મહાન', 'માં', 'માંગતો', 'માંગુ', 'માઇલ્ડ્યુ', 'માઇલ્ડ્યુથી', 'માઇલ્ડ્યુના', 'માઇલ્ડ્યુનું', 'માટી', 'માટે', 'માણસ', 'માણસની', 'માનધન', 'માનવ', 'માની', 'મારા', 'મારી', 'મારે', 'માર્કેટ્સ', 'માસ્ટર', 'માહિતી', 'મિશન', 'મૂર્ખ', 'મે', 'મેટલેક્સિલ', 'મેટલેક્સિલનું', 'મેનૂને', 'મેનેજર', 'મેન્કોઝેબ', 'મેન્કોઝેબનું', 'મેળવી', 'મોકલો', 'યોજના', 'યોજનામાં', 'રકમનું', 'રહે', 'રહો', 'રહ્યા', 'રહ્યો', 'રાખવા', 'રાષ્ટ્રીય', 'રીતે', 'રેકોર્ડ', 'રોકવા', 'રોગો', 'રોગોના', 'રોબોટ', 'રોબોટ્સ', 'લઈ', 'લક્ષણો', 'લાઈવ', 'લાગતા', 'લીધુ', 'લીફ', 'લોન', 'વડાપ્રધાનની', 'વધુ', 'વરસાદ', 'વર્ણન', 'વર્ણવો', 'વસ્તુ', 'વસ્તુઓ', 'વસ્તુઓની', 'વાત', 'વારંવાર', 'વાસ્તવિક', 'વિકલ્પો', 'વિકલ્પોની', 'વિકાસ', 'વિતરણ', 'વિશે', 'વિશેષતાઓ', 'વિષે', 'વિસ્તાર', 'વીમા', 'વેટેબલ', 'વોટસન', 'વ્યક્તિ', 'વ્યવહાર', 'શકશો', 'શકાતી', 'શકું', 'શકો', 'શું', 'શુભ', 'શુભેચ્છાઓ', 'શેના', 'શેમાં', 'શોધી', 'શ્રેેેેષ્ઠ', 'સંચય', 'સંપર્ક', 'સંભાળો', 'સક્ષમ', 'સત્ર', 'સમજવા', 'સમય', 'સમસ્યાઓ', 'સમાપ્ત', 'સરકાર', 'સરકારની', 'સરકારી', 'સરસ', 'સલાહકાર', 'સલ્ફર', 'સલ્ફરનું', 'સહાય', 'સાંજ', 'સાચા', 'સાથે', 'સાબિત', 'સારા', 'સારી', 'સારું', 'સારો', 'સિંચાઈ', 'સીધા', 'સુધર', 'સુપ્રભાત', 'સુવિધા', 'સૂચિ', 'સેવા', 'સેવાઓમાં', 'સોઇલ', 'સોલ્યુશન', 'સૌથી', 'સ્ટ્રેપ્ટોસાયક્લાઇન', 'સ્ટ્રેપ્ટોસાયક્લાઇનનું', 'સ્પોટ', 'સ્પોટનું', 'સ્માર્ટ', 'સ્યા', 'હતો', 'હર', 'હલ', 'હવે', 'હા', 'હાય', 'હું', 'હે', 'હેરાન', 'હેલો', 'હેલ્થ']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize, lower each word and remove duplicates\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "# sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "# documents = combination between patterns and intents\n",
    "print (len(documents), \"documents\")\n",
    "# classes = intents\n",
    "print (len(classes), \"classes\", classes)\n",
    "# words = all words, vocabulary\n",
    "print (len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "pickle.dump(words,open('words.pkl','wb'))\n",
    "pickle.dump(classes,open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9a640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data created\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_24868\\3106589240.py:24: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # lemmatize each word - create base word, in attempt to represent related words\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array with 1, if word match found in current pattern\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "# create train and test lists. X - patterns, Y - intents\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "print(\"Training data created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37f7af1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "47/47 [==============================] - 6s 14ms/step - loss: 3.8452 - accuracy: 0.0871\n",
      "Epoch 2/80\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 3.5527 - accuracy: 0.1429\n",
      "Epoch 3/80\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 3.3448 - accuracy: 0.1457\n",
      "Epoch 4/80\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 3.2303 - accuracy: 0.1629\n",
      "Epoch 5/80\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 3.1460 - accuracy: 0.1829\n",
      "Epoch 6/80\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 3.0189 - accuracy: 0.2200\n",
      "Epoch 7/80\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 2.8863 - accuracy: 0.2157\n",
      "Epoch 8/80\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 2.7670 - accuracy: 0.2714\n",
      "Epoch 9/80\n",
      "47/47 [==============================] - 1s 22ms/step - loss: 2.6953 - accuracy: 0.2743\n",
      "Epoch 10/80\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 2.6054 - accuracy: 0.2814\n",
      "Epoch 11/80\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 2.4952 - accuracy: 0.3071\n",
      "Epoch 12/80\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 2.3845 - accuracy: 0.3614\n",
      "Epoch 13/80\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 2.2978 - accuracy: 0.3671\n",
      "Epoch 14/80\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 2.2663 - accuracy: 0.3586\n",
      "Epoch 15/80\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 2.1644 - accuracy: 0.4114\n",
      "Epoch 16/80\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 2.0741 - accuracy: 0.4186\n",
      "Epoch 17/80\n",
      "47/47 [==============================] - 1s 23ms/step - loss: 1.9856 - accuracy: 0.4443\n",
      "Epoch 18/80\n",
      "47/47 [==============================] - 1s 24ms/step - loss: 1.9647 - accuracy: 0.4329\n",
      "Epoch 19/80\n",
      "47/47 [==============================] - 1s 21ms/step - loss: 1.9014 - accuracy: 0.4729\n",
      "Epoch 20/80\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 1.7847 - accuracy: 0.4843\n",
      "Epoch 21/80\n",
      "47/47 [==============================] - 1s 26ms/step - loss: 1.7270 - accuracy: 0.4971\n",
      "Epoch 22/80\n",
      "47/47 [==============================] - 1s 22ms/step - loss: 1.6571 - accuracy: 0.5329\n",
      "Epoch 23/80\n",
      "47/47 [==============================] - 1s 23ms/step - loss: 1.6478 - accuracy: 0.5286\n",
      "Epoch 24/80\n",
      "47/47 [==============================] - 1s 26ms/step - loss: 1.5516 - accuracy: 0.5629\n",
      "Epoch 25/80\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 1.4923 - accuracy: 0.5843\n",
      "Epoch 26/80\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 1.4650 - accuracy: 0.5829\n",
      "Epoch 27/80\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 1.4021 - accuracy: 0.5871\n",
      "Epoch 28/80\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 1.4340 - accuracy: 0.5929\n",
      "Epoch 29/80\n",
      "47/47 [==============================] - 1s 14ms/step - loss: 1.3506 - accuracy: 0.6071\n",
      "Epoch 30/80\n",
      "47/47 [==============================] - 1s 11ms/step - loss: 1.2934 - accuracy: 0.6286\n",
      "Epoch 31/80\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 1.2476 - accuracy: 0.6286\n",
      "Epoch 32/80\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 1.1782 - accuracy: 0.6657\n",
      "Epoch 33/80\n",
      "47/47 [==============================] - 1s 22ms/step - loss: 1.1463 - accuracy: 0.6757\n",
      "Epoch 34/80\n",
      "47/47 [==============================] - 1s 21ms/step - loss: 1.1203 - accuracy: 0.6800\n",
      "Epoch 35/80\n",
      "47/47 [==============================] - 2s 30ms/step - loss: 1.1121 - accuracy: 0.6686\n",
      "Epoch 36/80\n",
      "47/47 [==============================] - 2s 35ms/step - loss: 1.1041 - accuracy: 0.6871\n",
      "Epoch 37/80\n",
      "47/47 [==============================] - 1s 22ms/step - loss: 1.0746 - accuracy: 0.6871\n",
      "Epoch 38/80\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 0.9925 - accuracy: 0.6986\n",
      "Epoch 39/80\n",
      "47/47 [==============================] - 1s 23ms/step - loss: 0.9964 - accuracy: 0.7186\n",
      "Epoch 40/80\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.9583 - accuracy: 0.7186\n",
      "Epoch 41/80\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.9370 - accuracy: 0.7443\n",
      "Epoch 42/80\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.8768 - accuracy: 0.7500\n",
      "Epoch 43/80\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.9079 - accuracy: 0.7214\n",
      "Epoch 44/80\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.8759 - accuracy: 0.7529\n",
      "Epoch 45/80\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.8390 - accuracy: 0.7457\n",
      "Epoch 46/80\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.8391 - accuracy: 0.7557\n",
      "Epoch 47/80\n",
      "47/47 [==============================] - 1s 12ms/step - loss: 0.7976 - accuracy: 0.7743\n",
      "Epoch 48/80\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.7235 - accuracy: 0.7857\n",
      "Epoch 49/80\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.7698 - accuracy: 0.7829\n",
      "Epoch 50/80\n",
      "47/47 [==============================] - 1s 26ms/step - loss: 0.7098 - accuracy: 0.7929\n",
      "Epoch 51/80\n",
      "47/47 [==============================] - 2s 33ms/step - loss: 0.7570 - accuracy: 0.7743\n",
      "Epoch 52/80\n",
      "47/47 [==============================] - 1s 21ms/step - loss: 0.6930 - accuracy: 0.7986\n",
      "Epoch 53/80\n",
      "47/47 [==============================] - 2s 39ms/step - loss: 0.6634 - accuracy: 0.8029\n",
      "Epoch 54/80\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.6748 - accuracy: 0.7900\n",
      "Epoch 55/80\n",
      "47/47 [==============================] - 1s 25ms/step - loss: 0.6494 - accuracy: 0.8100\n",
      "Epoch 56/80\n",
      "47/47 [==============================] - 1s 23ms/step - loss: 0.6679 - accuracy: 0.8029\n",
      "Epoch 57/80\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 0.6651 - accuracy: 0.8029\n",
      "Epoch 58/80\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 0.6505 - accuracy: 0.8100\n",
      "Epoch 59/80\n",
      "47/47 [==============================] - 1s 21ms/step - loss: 0.6176 - accuracy: 0.8086\n",
      "Epoch 60/80\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.5756 - accuracy: 0.8314\n",
      "Epoch 61/80\n",
      "47/47 [==============================] - 1s 13ms/step - loss: 0.5511 - accuracy: 0.8529\n",
      "Epoch 62/80\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.5712 - accuracy: 0.8400\n",
      "Epoch 63/80\n",
      "47/47 [==============================] - 1s 15ms/step - loss: 0.5473 - accuracy: 0.8343\n",
      "Epoch 64/80\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.5326 - accuracy: 0.8400\n",
      "Epoch 65/80\n",
      "47/47 [==============================] - 1s 20ms/step - loss: 0.5313 - accuracy: 0.8386\n",
      "Epoch 66/80\n",
      "47/47 [==============================] - 1s 19ms/step - loss: 0.5294 - accuracy: 0.8471\n",
      "Epoch 67/80\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.5184 - accuracy: 0.8429\n",
      "Epoch 68/80\n",
      "47/47 [==============================] - 1s 25ms/step - loss: 0.4914 - accuracy: 0.8471\n",
      "Epoch 69/80\n",
      "47/47 [==============================] - 1s 16ms/step - loss: 0.5443 - accuracy: 0.8214\n",
      "Epoch 70/80\n",
      "47/47 [==============================] - 1s 17ms/step - loss: 0.5117 - accuracy: 0.8414\n",
      "Epoch 71/80\n",
      "47/47 [==============================] - 1s 19ms/step - loss: 0.5032 - accuracy: 0.8571\n",
      "Epoch 72/80\n",
      "47/47 [==============================] - 1s 18ms/step - loss: 0.4616 - accuracy: 0.8571\n",
      "Epoch 73/80\n",
      "47/47 [==============================] - 1s 28ms/step - loss: 0.4364 - accuracy: 0.8729\n",
      "Epoch 74/80\n",
      "47/47 [==============================] - 1s 27ms/step - loss: 0.4652 - accuracy: 0.8671\n",
      "Epoch 75/80\n",
      "47/47 [==============================] - 1s 19ms/step - loss: 0.4569 - accuracy: 0.8571\n",
      "Epoch 76/80\n",
      "47/47 [==============================] - 1s 19ms/step - loss: 0.4404 - accuracy: 0.8743\n",
      "Epoch 77/80\n",
      "47/47 [==============================] - 1s 23ms/step - loss: 0.4305 - accuracy: 0.8657\n",
      "Epoch 78/80\n",
      "47/47 [==============================] - 1s 29ms/step - loss: 0.4160 - accuracy: 0.8671\n",
      "Epoch 79/80\n",
      "47/47 [==============================] - 1s 25ms/step - loss: 0.3990 - accuracy: 0.8814\n",
      "Epoch 80/80\n",
      "47/47 [==============================] - 1s 27ms/step - loss: 0.4118 - accuracy: 0.8800\n",
      "model created\n"
     ]
    }
   ],
   "source": [
    "# Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons\n",
    "# equal to number of intents to predict output intent with softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "# Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model\n",
    "# sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "sgd = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#fitting and saving the model \n",
    "hist = model.fit(np.array(train_x), np.array(train_y), epochs=80, batch_size=15, verbose=1)\n",
    "model.save('chatbot_model.h5', hist)\n",
    "\n",
    "print(\"model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9189ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import load_model\n",
    "model = load_model('chatbot_model.h5')\n",
    "import json\n",
    "import random\n",
    "intents = json.loads(open('intents.json',encoding = \"utf-8\").read())\n",
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a15acdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern - split words into array\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word - create short form for word\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words - matrix of N words, vocabulary matrix\n",
    "    bag = [0]*len(words) \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                # assign 1 if current word is in the vocabulary position\n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "    return(np.array(bag))\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # filter out predictions below a threshold\n",
    "    p = bow(sentence, words,show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c63ef7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResponse(ints, intents_json):\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if(i['tag']== tag):\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response(text):\n",
    "    ints = predict_class(text, model)\n",
    "    res = getResponse(ints, intents)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c09cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf6c2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ગુડ મોર્નીંગ'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class difflanguage:\n",
    "    def lang(self, Language):\n",
    "\n",
    "        default = \"English\"\n",
    "\n",
    "        return getattr(self, 'case_' + str(Language), lambda: default)()\n",
    "\n",
    "    def case_1(self):\n",
    "        translator= Translator(from_lang=\"english\",to_lang=(\"gujarati\"))\n",
    "        translation = translator.translate(\"Good Morning\")\n",
    "        return translation\n",
    "\n",
    " \n",
    "\n",
    "    def case_2(self):\n",
    "        translator= Translator(from_lang=\"english\",to_lang=(\"marathi\"))\n",
    "        translation = translator.translate(\"Good Morning\")\n",
    "        return translation\n",
    "\n",
    "\n",
    "\n",
    "    def case_3(self):\n",
    "        translator= Translator(from_lang=\"english\",to_lang=(\"german\"))\n",
    "        translation = translator.translate(\"Good Morning\")\n",
    "        return translation\n",
    "\n",
    " \n",
    "\n",
    "    def case_4(self):\n",
    "        translator= Translator(from_lang=\"english\",to_lang=(\"french\"))\n",
    "        translation = translator.translate(\"Good Morning\")\n",
    "        return translation\n",
    "\n",
    "\n",
    "\n",
    "    def case_5(self):\n",
    "        translator= Translator(from_lang=\"english\",to_lang=(\"tamil\"))\n",
    "        translation = translator.translate(\"Good Morning\")\n",
    "        return translation\n",
    "    \n",
    "    \n",
    "   \n",
    "my_switch = difflanguage()\n",
    "my_switch.lang(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04e5942a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 350ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "\n",
    "\n",
    "def send():\n",
    "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
    "    EntryBox.delete(\"0.0\",END)\n",
    "    if msg != '':\n",
    "        ChatLog.config(state=NORMAL)\n",
    "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
    "        ChatLog.config(foreground=\"Black\", font=(\"Verdana\", 14))\n",
    "        res = chatbot_response(msg)\n",
    "        my_switch = difflanguage()\n",
    "        ChatLog.insert(END, \"EDITH: \" + res + '\\n\\n')\n",
    "        ChatLog.config(state=DISABLED)\n",
    "        ChatLog.yview(END)\n",
    "\n",
    "base = tk.Tk()\n",
    "base.title(\"Multilingual Chatbot\")\n",
    "base.geometry(\"700x800\")\n",
    "base.resizable(width=FALSE, height=FALSE)\n",
    "ChatLog = Text(base, bd=0, bg=\"#8E8D8D\", height=\"10\", width=\"60\", font=\"Arial\")\n",
    "ChatLog.config(state=DISABLED)\n",
    "scrollbar = Scrollbar(base, command=ChatLog.yview, cursor=\"heart\")\n",
    "ChatLog['yscrollcommand'] = scrollbar.set\n",
    "SendButton = Button(base, font=(\"Verdana\",18,'bold'), text=\"Send\", width=\"12\", height=5,\n",
    "                    bd=0, bg=\"#5CA1CB\", activebackground=\"#3c9d9b\",fg='#ffffff',\n",
    "                    command= send )\n",
    "EntryBox = Text(base, bd=0, bg=\"WHITE\",width=\"29\", height=\"5\", font=(\"Verdana\",18))\n",
    "scrollbar.place(x=682,y=6, height=800)\n",
    "ChatLog.place(x=6,y=6, height=673, width=800)\n",
    "EntryBox.place(x=301, y=679, height=120, width=380)\n",
    "SendButton.place(x=6, y=679, height=120,width=300)\n",
    "base.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763a265d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac9b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13bee45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e19395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
